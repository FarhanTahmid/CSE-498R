{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import english_nlp\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model\n",
    "class CyberBullyingClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CyberBullyingClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        probability = self.sigmoid(logits)\n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CyberBullyingClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model instance\n",
    "model = CyberBullyingClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('G:/OneDrive - northsouth.edu/CODES/PROJECTS/PROJECT - Bullishield Models/Created Models/english_bert.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data:  (14490, 4)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "\n",
    "try:\n",
    "    all_data = pd.read_csv('english_hate_speech.csv')\n",
    "    all_data.head()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "print(\"Shape of the data: \", all_data.shape) #inspecting the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisation successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I give permission for Wikipedia to use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aggression_parsed_dataset_subset.csv</td>\n",
       "      <td>give permission Wikipedia use material Nancy S...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>` October 2010 (UTC) :::::It does look that wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxicity_parsed_dataset_subset.csv</td>\n",
       "      <td>October 2010 UTC look way merger sounds like C...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sounds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dcm81: #killerblondes ?? Have the producer...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>RT dcm81 killerblondes producers lost plot eye...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producers, lost, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ShaofHappiness COME SAY HI TO ME THIS EVENING...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>ShaofHappiness COME SAY HI EVENING ILU</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ManhattaKnight I mean he's gay, but he uses g...</td>\n",
       "      <td>1</td>\n",
       "      <td>cyberbullying_tweets_subset.csv</td>\n",
       "      <td>ManhattaKnight mean hes gay uses gendered slur...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  oh_label  \\\n",
       "0     I give permission for Wikipedia to use the ...         0   \n",
       "1  ` October 2010 (UTC) :::::It does look that wa...         0   \n",
       "2  RT @dcm81: #killerblondes ?? Have the producer...         0   \n",
       "3  @ShaofHappiness COME SAY HI TO ME THIS EVENING...         0   \n",
       "4  @ManhattaKnight I mean he's gay, but he uses g...         1   \n",
       "\n",
       "                                Dataset  \\\n",
       "0  aggression_parsed_dataset_subset.csv   \n",
       "1    toxicity_parsed_dataset_subset.csv   \n",
       "2     twitter_parsed_dataset_subset.csv   \n",
       "3     twitter_parsed_dataset_subset.csv   \n",
       "4       cyberbullying_tweets_subset.csv   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  give permission Wikipedia use material Nancy S...   \n",
       "1  October 2010 UTC look way merger sounds like C...   \n",
       "2  RT dcm81 killerblondes producers lost plot eye...   \n",
       "3             ShaofHappiness COME SAY HI EVENING ILU   \n",
       "4  ManhattaKnight mean hes gay uses gendered slur...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [give, permission, wikipedia, use, material, n...  \n",
       "1  [october, 2010, utc, look, way, merger, sounds...  \n",
       "2  [rt, dcm81, killerblondes, producers, lost, pl...  \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]  \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise_text(data):\n",
    "    \"\"\"\n",
    "    Tokenise the text in the clean_text column\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # label changed to a float when imported, changing it back\n",
    "        data['oh_label'] = data['oh_label'].astype(int)\n",
    "        data['clean_text'] = data['clean_text'].astype(str)\n",
    "\n",
    "        # also lowercasing all words\n",
    "        data['tokens'] = data['clean_text'].apply(lambda x: [word.lower() for word in x.split()]) #ensures all text is lowercase\n",
    "        print(\"Tokenisation successful\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenisation error: {e}\")\n",
    "        return None\n",
    "\n",
    "all_data = tokenise_text(all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I give permission for Wikipedia to use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aggression_parsed_dataset_subset.csv</td>\n",
       "      <td>give permission Wikipedia use material Nancy S...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>` October 2010 (UTC) :::::It does look that wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxicity_parsed_dataset_subset.csv</td>\n",
       "      <td>October 2010 UTC look way merger sounds like C...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sounds...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sound,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dcm81: #killerblondes ?? Have the producer...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>RT dcm81 killerblondes producers lost plot eye...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producers, lost, pl...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producer, lost, plo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ShaofHappiness COME SAY HI TO ME THIS EVENING...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>ShaofHappiness COME SAY HI EVENING ILU</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ManhattaKnight I mean he's gay, but he uses g...</td>\n",
       "      <td>1</td>\n",
       "      <td>cyberbullying_tweets_subset.csv</td>\n",
       "      <td>ManhattaKnight mean hes gay uses gendered slur...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  oh_label  \\\n",
       "0     I give permission for Wikipedia to use the ...         0   \n",
       "1  ` October 2010 (UTC) :::::It does look that wa...         0   \n",
       "2  RT @dcm81: #killerblondes ?? Have the producer...         0   \n",
       "3  @ShaofHappiness COME SAY HI TO ME THIS EVENING...         0   \n",
       "4  @ManhattaKnight I mean he's gay, but he uses g...         1   \n",
       "\n",
       "                                Dataset  \\\n",
       "0  aggression_parsed_dataset_subset.csv   \n",
       "1    toxicity_parsed_dataset_subset.csv   \n",
       "2     twitter_parsed_dataset_subset.csv   \n",
       "3     twitter_parsed_dataset_subset.csv   \n",
       "4       cyberbullying_tweets_subset.csv   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  give permission Wikipedia use material Nancy S...   \n",
       "1  October 2010 UTC look way merger sounds like C...   \n",
       "2  RT dcm81 killerblondes producers lost plot eye...   \n",
       "3             ShaofHappiness COME SAY HI EVENING ILU   \n",
       "4  ManhattaKnight mean hes gay uses gendered slur...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, 2010, utc, look, way, merger, sounds...   \n",
       "2  [rt, dcm81, killerblondes, producers, lost, pl...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [give, permission, wikipedia, use, material, n...  \n",
       "1  [october, 2010, utc, look, way, merger, sound,...  \n",
       "2  [rt, dcm81, killerblondes, producer, lost, plo...  \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]  \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def lemmatize_text(data):\n",
    "    \"\"\"\n",
    "    Lemmatises the tesxt data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lemm = WordNetLemmatizer() #using the inbuilt lemmatisation function\n",
    "\n",
    "    # Lemmatize all words\n",
    "        data['lemmatized'] = data['tokens'].apply(lambda x: [lemm.lemmatize(word) for word in x])\n",
    "        print(\"Lemmatisation successful\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during lemmatisation: {e}\")\n",
    "        return data\n",
    "\n",
    "all_data=lemmatize_text(data=all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_no_numbers</th>\n",
       "      <th>lemmatized_clean</th>\n",
       "      <th>tokenized_clean</th>\n",
       "      <th>string_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I give permission for Wikipedia to use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aggression_parsed_dataset_subset.csv</td>\n",
       "      <td>give permission Wikipedia use material Nancy S...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>give permission wikipedia use material nancy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>` October 2010 (UTC) :::::It does look that wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxicity_parsed_dataset_subset.csv</td>\n",
       "      <td>October 2010 UTC look way merger sounds like C...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sounds...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sound,...</td>\n",
       "      <td>[october, utc, look, way, merger, sound, like,...</td>\n",
       "      <td>[october, utc, look, way, merger, sound, like,...</td>\n",
       "      <td>[october, utc, look, way, merger, sounds, like...</td>\n",
       "      <td>october utc look way merger sounds like cathol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dcm81: #killerblondes ?? Have the producer...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>RT dcm81 killerblondes producers lost plot eye...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producers, lost, pl...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producer, lost, plo...</td>\n",
       "      <td>[rt, killerblondes, producer, lost, plot, eye,...</td>\n",
       "      <td>[rt, killerblondes, producer, lost, plot, eye,...</td>\n",
       "      <td>[rt, killerblondes, producers, lost, plot, eye...</td>\n",
       "      <td>rt killerblondes producers lost plot eyes fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ShaofHappiness COME SAY HI TO ME THIS EVENING...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>ShaofHappiness COME SAY HI EVENING ILU</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>shaofhappiness come say hi evening ilu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ManhattaKnight I mean he's gay, but he uses g...</td>\n",
       "      <td>1</td>\n",
       "      <td>cyberbullying_tweets_subset.csv</td>\n",
       "      <td>ManhattaKnight mean hes gay uses gendered slur...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "      <td>manhattaknight mean hes gay uses gendered slur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  oh_label  \\\n",
       "0     I give permission for Wikipedia to use the ...         0   \n",
       "1  ` October 2010 (UTC) :::::It does look that wa...         0   \n",
       "2  RT @dcm81: #killerblondes ?? Have the producer...         0   \n",
       "3  @ShaofHappiness COME SAY HI TO ME THIS EVENING...         0   \n",
       "4  @ManhattaKnight I mean he's gay, but he uses g...         1   \n",
       "\n",
       "                                Dataset  \\\n",
       "0  aggression_parsed_dataset_subset.csv   \n",
       "1    toxicity_parsed_dataset_subset.csv   \n",
       "2     twitter_parsed_dataset_subset.csv   \n",
       "3     twitter_parsed_dataset_subset.csv   \n",
       "4       cyberbullying_tweets_subset.csv   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  give permission Wikipedia use material Nancy S...   \n",
       "1  October 2010 UTC look way merger sounds like C...   \n",
       "2  RT dcm81 killerblondes producers lost plot eye...   \n",
       "3             ShaofHappiness COME SAY HI EVENING ILU   \n",
       "4  ManhattaKnight mean hes gay uses gendered slur...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, 2010, utc, look, way, merger, sounds...   \n",
       "2  [rt, dcm81, killerblondes, producers, lost, pl...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, 2010, utc, look, way, merger, sound,...   \n",
       "2  [rt, dcm81, killerblondes, producer, lost, plo...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...   \n",
       "\n",
       "                               lemmatized_no_numbers  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, utc, look, way, merger, sound, like,...   \n",
       "2  [rt, killerblondes, producer, lost, plot, eye,...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...   \n",
       "\n",
       "                                    lemmatized_clean  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, utc, look, way, merger, sound, like,...   \n",
       "2  [rt, killerblondes, producer, lost, plot, eye,...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...   \n",
       "\n",
       "                                     tokenized_clean  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, utc, look, way, merger, sounds, like...   \n",
       "2  [rt, killerblondes, producers, lost, plot, eye...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...   \n",
       "\n",
       "                                    string_tokenized  \n",
       "0  give permission wikipedia use material nancy s...  \n",
       "1  october utc look way merger sounds like cathol...  \n",
       "2  rt killerblondes producers lost plot eyes fail...  \n",
       "3             shaofhappiness come say hi evening ilu  \n",
       "4  manhattaknight mean hes gay uses gendered slur...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(word_list):\n",
    "    \"\"\"\n",
    "    removees any numbers from the text\n",
    "    \"\"\"\n",
    "    return [word for word in word_list if not bool(re.search(r'\\d', word))]\n",
    "\n",
    "# Function to remove URLs from a list of words\n",
    "def remove_urls(word_list):\n",
    "    \"\"\"\n",
    "    Removes any URLs from the text\n",
    "    \"\"\"\n",
    "    return [word for word in word_list if not (word.startswith('http') or word.startswith('www') or word.startswith('https'))]\n",
    "\n",
    "all_data['lemmatized_no_numbers'] = all_data['lemmatized'].apply(remove_numbers)\n",
    "all_data['lemmatized_clean'] = all_data['lemmatized_no_numbers'].apply(remove_urls)\n",
    "\n",
    "all_data['tokenized_clean']=all_data['tokens'].apply(remove_numbers)\n",
    "all_data['tokenized_clean']=all_data['tokenized_clean'].apply(remove_urls)\n",
    "all_data['string_tokenized']=all_data['tokenized_clean'].apply(' '.join)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and temp (validation + test)\n",
    "train_data, temp_data = train_test_split(all_data, test_size=0.4, random_state=42)  # 40% of data for temp\n",
    "\n",
    "# Split temp into validation and test\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  # 50% of temp for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_texts(texts, max_length):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokenized_texts\n",
    "\n",
    "max_length=128\n",
    "\n",
    "train_tokenized_texts = tokenize_texts(train_data['string_tokenized'].tolist(), max_length)\n",
    "test_tokenized_texts = tokenize_texts(test_data['string_tokenized'].tolist(), max_length)\n",
    "val_tokenized_texts = tokenize_texts(val_data['string_tokenized'].tolist(), max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test inputs and labels\n",
    "train_inputs = {\n",
    "    'input_ids': train_tokenized_texts['input_ids'],\n",
    "    'attention_mask': train_tokenized_texts['attention_mask']\n",
    "}\n",
    "\n",
    "train_labels = torch.tensor(train_data['oh_label'].values,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_inputs = {\n",
    "    'input_ids': test_tokenized_texts['input_ids'],\n",
    "    'attention_mask': test_tokenized_texts['attention_mask']\n",
    "}\n",
    "\n",
    "test_labels = torch.tensor(test_data['oh_label'].values,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "val_inputs = {\n",
    "    'input_ids': val_tokenized_texts['input_ids'],\n",
    "    'attention_mask': val_tokenized_texts['attention_mask']\n",
    "}\n",
    "\n",
    "val_labels = torch.tensor(val_data['oh_label'].values,dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, inputs, labels, device, batch_size=32):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(inputs['input_ids']), batch_size):\n",
    "            batch_inputs = {key: val[i:i+batch_size].to(device) for key, val in inputs.items()}\n",
    "            batch_labels = labels[i:i+batch_size].to(device)\n",
    "\n",
    "            # Get model output, assuming it returns logits or probabilities directly\n",
    "            outputs = model(**batch_inputs)\n",
    "\n",
    "            # Convert model outputs to predictions (assuming binary classification)\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    preds = np.array(preds) >= 0.5  # Adjust this threshold if needed\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds)\n",
    "    recall = recall_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9648\n",
      "Validation Precision: 0.9375\n",
      "Validation Recall: 0.9192\n",
      "Validation F1 Score: 0.9283\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(model, val_inputs, val_labels, device)\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(f'Validation Precision: {val_precision:.4f}')\n",
    "print(f'Validation Recall: {val_recall:.4f}')\n",
    "print(f'Validation F1 Score: {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9651\n",
      "Test Precision: 0.9380\n",
      "Test Recall: 0.9294\n",
      "Test F1 Score: 0.9337\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(model, test_inputs, test_labels, device)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "print(f'Test Precision: {test_precision:.4f}')\n",
    "print(f'Test Recall: {test_recall:.4f}')\n",
    "print(f'Test F1 Score: {test_f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
