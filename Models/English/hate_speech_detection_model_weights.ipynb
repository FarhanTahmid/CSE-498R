{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def install_packages():\n",
    "#     packages = [\n",
    "#         \"nltk\",\n",
    "#         \"scikit-learn\",\n",
    "#         \"pyLDAvis\",\n",
    "#         \"gensim\",\n",
    "#         \"matplotlib\",\n",
    "#         \"wordcloud\",\n",
    "#         \"seaborn\",\n",
    "#         \"pandas\",\n",
    "#         \"numpy\",\n",
    "#     ]\n",
    "#     for package in packages:\n",
    "#         os.system(f\"pip install {package}\")\n",
    "\n",
    "# install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyLDAvis\n",
    "import numpy as np\n",
    "import time\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "try:\n",
    "    all_data = pd.read_csv('english_hate_speech.csv')\n",
    "    all_data.head()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "print(\"Shape of the data: \", all_data.shape) #inspecting the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_text(data):\n",
    "    \"\"\"\n",
    "    Tokenise the text in the clean_text column\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # label changed to a float when imported, changing it back\n",
    "        data['oh_label'] = data['oh_label'].astype(int)\n",
    "        data['clean_text'] = data['clean_text'].astype(str)\n",
    "\n",
    "        # also lowercasing all words\n",
    "        data['tokens'] = data['clean_text'].apply(lambda x: [word.lower() for word in x.split()]) #ensures all text is lowercase\n",
    "        print(\"Tokenisation successful\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenisation error: {e}\")\n",
    "        return None\n",
    "\n",
    "all_data = tokenise_text(all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_text(data):\n",
    "    \"\"\"\n",
    "    Lemmatises the tesxt data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lemm = WordNetLemmatizer() #using the inbuilt lemmatisation function\n",
    "\n",
    "    # Lemmatize all words\n",
    "        data['lemmatized'] = data['tokens'].apply(lambda x: [lemm.lemmatize(word) for word in x])\n",
    "        print(\"Lemmatisation successful\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during lemmatisation: {e}\")\n",
    "        return data\n",
    "\n",
    "all_data=lemmatize_text(data=all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(word_list):\n",
    "    \"\"\"\n",
    "    removees any numbers from the text\n",
    "    \"\"\"\n",
    "    return [word for word in word_list if not bool(re.search(r'\\d', word))]\n",
    "\n",
    "# Function to remove URLs from a list of words\n",
    "def remove_urls(word_list):\n",
    "    \"\"\"\n",
    "    Removes any URLs from the text\n",
    "    \"\"\"\n",
    "    return [word for word in word_list if not (word.startswith('http') or word.startswith('www') or word.startswith('https'))]\n",
    "\n",
    "all_data['lemmatized_no_numbers'] = all_data['lemmatized'].apply(remove_numbers)\n",
    "all_data['lemmatized_clean'] = all_data['lemmatized_no_numbers'].apply(remove_urls)\n",
    "\n",
    "all_data['tokenized_clean']=all_data['tokens'].apply(remove_numbers)\n",
    "all_data['tokenized_clean']=all_data['tokenized_clean'].apply(remove_urls)\n",
    "all_data['string_tokenized']=all_data['tokenized_clean'].apply(' '.join)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where 'oh_label' is 1\n",
    "count_label_1 = all_data[all_data['oh_label'] == 1].shape[0]\n",
    "\n",
    "# Count the number of rows where 'oh_label' is 0\n",
    "count_label_0 = all_data[all_data['oh_label'] == 0].shape[0]\n",
    "\n",
    "print(f\"Number of rows with oh_label 1: {count_label_1}\")\n",
    "print(f\"Number of rows with oh_label 0: {count_label_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the lemmatized strings\n",
    "def tokenize_texts(texts, max_length):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokenized_texts\n",
    "\n",
    "# Tokenize train and test texts\n",
    "\n",
    "max_length = 128\n",
    "train_tokenized_texts = tokenize_texts(train_data['string_tokenized'].tolist(), max_length)\n",
    "test_tokenized_texts = tokenize_texts(test_data['string_tokenized'].tolist(), max_length)\n",
    "\n",
    "# Prepare train and test inputs and labels\n",
    "train_inputs = {\n",
    "    'input_ids': train_tokenized_texts['input_ids'],\n",
    "    'attention_mask': train_tokenized_texts['attention_mask']\n",
    "}\n",
    "train_labels = torch.tensor(train_data['oh_label'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_inputs = {\n",
    "    'input_ids': test_tokenized_texts['input_ids'],\n",
    "    'attention_mask': test_tokenized_texts['attention_mask']\n",
    "}\n",
    "test_labels = torch.tensor(test_data['oh_label'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Transformer model\n",
    "class CyberBullyingClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CyberBullyingClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        probability = self.sigmoid(logits)\n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "model = CyberBullyingClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(y):\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    class_weights = []\n",
    "\n",
    "    for class_count in class_counts:\n",
    "        class_weight = total_samples / (2.0 * class_count)\n",
    "        class_weights.append( class_weight)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class weights to a tensor\n",
    "weights = calculate_class_weights(train_labels.numpy())\n",
    "print(weights)\n",
    "class_weights = torch.tensor([weights[1]/weights[0]],dtype=torch.float32).to(device=device)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "epochs = 100\n",
    "total_steps = len(train_inputs['input_ids']) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Defining loss function\n",
    "criterion = nn.BCELoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_inputs, train_labels, criterion, optimizer, scheduler, device, batch_size=32, epochs=epochs):\n",
    "    model.train()\n",
    "    best_loss=float('inf')\n",
    "    wait=0\n",
    "    epsilon=1e-8\n",
    "    patience_counter=3\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, len(train_inputs['input_ids']), batch_size):\n",
    "            inputs = {key: val[i:i+batch_size].to(device) for key, val in train_inputs.items()}\n",
    "            labels = train_labels[i:i+batch_size].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_size\n",
    "\n",
    "        epoch_loss = running_loss / len(train_labels)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        if epoch_loss<best_loss-epsilon:\n",
    "            best_loss=epoch_loss\n",
    "            wait=0\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait >=patience_counter:\n",
    "                print(f'Stopping early at epoch {epoch+1} due to insignificant loss change.')\n",
    "                break\n",
    "\n",
    "# Training the model\n",
    "train_model(model, train_inputs, train_labels, criterion, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate_model(model, test_inputs, test_labels, device, batch_size=32):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_inputs['input_ids']), batch_size):\n",
    "            inputs = {key: val[i:i+batch_size].to(device) for key, val in test_inputs.items()}\n",
    "            labels = test_labels[i:i+batch_size].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    preds = np.array(preds) >= 0.5\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = evaluate_model(model, test_inputs, test_labels, device)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), '/home/ara2/Desktop/Farhan_Bullishield_CSE498R/Created Models/english_bert_class_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def sentence_preprocessing(sentence_list):\n",
    "    '''Returns a list of lemmatized texts upon calling'''\n",
    "    \n",
    "    # initialize stopwords for english\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    preprocessed_sentence_list=[]\n",
    "    for sentence in sentence_list:\n",
    "        # keep the type str\n",
    "        sentence=str(sentence)\n",
    "        \n",
    "        # lowercasing the sentence and tokenising it\n",
    "        words= word_tokenize(sentence.lower())\n",
    "        \n",
    "        # Filter out stopwords\n",
    "        filtered_tokenized_words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # lemmatize texts\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        lemmatized_words=[lemmatizer.lemmatize(word) for word in filtered_tokenized_words] \n",
    "        \n",
    "\n",
    "        # remove urls and numbers\n",
    "        clean_sentence=remove_numbers(word_list=lemmatized_words)\n",
    "        preprocessed_sentence=remove_urls(word_list=clean_sentence)\n",
    "        preprocessed_sentence_list.append(preprocessed_sentence)\n",
    "    \n",
    "    return preprocessed_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(preprocessed_sentence_list):\n",
    "    \n",
    "    sentiment_score_list=[]\n",
    "    intensity_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for i in range(len(preprocessed_sentence_list)):\n",
    "        # convert list of words to string\n",
    "        converted_string=\" \".join(preprocessed_sentence_list[i])\n",
    "        # analyze sentiment\n",
    "        \n",
    "        sentiment_score=intensity_analyser.polarity_scores(converted_string)['compound']\n",
    "        sentiment_score_list.append(sentiment_score)\n",
    "        \n",
    "    return sentiment_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check score\n",
    "\n",
    "sentence_list=[\"Alvi is really a good motherfucker\",\"Fuck you motherfucker\",\"I have loved you all my life\"]\n",
    "\n",
    "preprocessed_list=sentence_preprocessing(sentence_list)\n",
    "sentiment_list=analyze_sentiment(preprocessed_list)\n",
    "\n",
    "for i in range(len(sentence_list)):\n",
    "    print(f\"Sentence:{sentence_list[i]}\\nSentiment Score:{sentiment_list[i]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_custom_texts(model, tokenizer, texts, device):\n",
    "    model.eval()\n",
    "    tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {key: val.to(device) for key, val in tokenized_texts.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = outputs.cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "# Predict custom texts\n",
    "custom_probabilities = predict_custom_texts(model, tokenizer, sentence_list, device)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "custom_pred_labels = [1 if prob >= 0.5 else 0 for prob in custom_probabilities]\n",
    "\n",
    "# Print predictions\n",
    "for text, label in zip(sentence_list, custom_pred_labels):\n",
    "    if label == 1:\n",
    "        print(f'Text: \"{text}\" is predicted as cyberbullying.')\n",
    "    else:\n",
    "        print(f'Text: \"{text}\" is predicted as not cyberbullying.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
