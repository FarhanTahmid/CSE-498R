{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def install_packages():\n",
    "#     packages = [\n",
    "#         \"nltk\",\n",
    "#         \"scikit-learn\",\n",
    "#         \"pyLDAvis\",\n",
    "#         \"gensim\",\n",
    "#         \"matplotlib\",\n",
    "#         \"wordcloud\",\n",
    "#         \"seaborn\",\n",
    "#         \"pandas\",\n",
    "#         \"numpy\",\n",
    "#     ]\n",
    "#     for package in packages:\n",
    "#         os.system(f\"pip install {package}\")\n",
    "\n",
    "# install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "g:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyLDAvis\n",
    "import numpy as np\n",
    "import time\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data:  (14490, 4)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "\n",
    "try:\n",
    "    all_data = pd.read_csv('english_hate_speech.csv')\n",
    "    all_data.head()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "print(\"Shape of the data: \", all_data.shape) #inspecting the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisation successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I give permission for Wikipedia to use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aggression_parsed_dataset_subset.csv</td>\n",
       "      <td>give permission Wikipedia use material Nancy S...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>` October 2010 (UTC) :::::It does look that wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxicity_parsed_dataset_subset.csv</td>\n",
       "      <td>October 2010 UTC look way merger sounds like C...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sounds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dcm81: #killerblondes ?? Have the producer...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>RT dcm81 killerblondes producers lost plot eye...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producers, lost, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ShaofHappiness COME SAY HI TO ME THIS EVENING...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>ShaofHappiness COME SAY HI EVENING ILU</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ManhattaKnight I mean he's gay, but he uses g...</td>\n",
       "      <td>1</td>\n",
       "      <td>cyberbullying_tweets_subset.csv</td>\n",
       "      <td>ManhattaKnight mean hes gay uses gendered slur...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  oh_label  \\\n",
       "0     I give permission for Wikipedia to use the ...         0   \n",
       "1  ` October 2010 (UTC) :::::It does look that wa...         0   \n",
       "2  RT @dcm81: #killerblondes ?? Have the producer...         0   \n",
       "3  @ShaofHappiness COME SAY HI TO ME THIS EVENING...         0   \n",
       "4  @ManhattaKnight I mean he's gay, but he uses g...         1   \n",
       "\n",
       "                                Dataset  \\\n",
       "0  aggression_parsed_dataset_subset.csv   \n",
       "1    toxicity_parsed_dataset_subset.csv   \n",
       "2     twitter_parsed_dataset_subset.csv   \n",
       "3     twitter_parsed_dataset_subset.csv   \n",
       "4       cyberbullying_tweets_subset.csv   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  give permission Wikipedia use material Nancy S...   \n",
       "1  October 2010 UTC look way merger sounds like C...   \n",
       "2  RT dcm81 killerblondes producers lost plot eye...   \n",
       "3             ShaofHappiness COME SAY HI EVENING ILU   \n",
       "4  ManhattaKnight mean hes gay uses gendered slur...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [give, permission, wikipedia, use, material, n...  \n",
       "1  [october, 2010, utc, look, way, merger, sounds...  \n",
       "2  [rt, dcm81, killerblondes, producers, lost, pl...  \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]  \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise_text(data):\n",
    "    \"\"\"\n",
    "    Tokenise the text in the clean_text column\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # label changed to a float when imported, changing it back\n",
    "        data['oh_label'] = data['oh_label'].astype(int)\n",
    "        data['clean_text'] = data['clean_text'].astype(str)\n",
    "\n",
    "        # also lowercasing all words\n",
    "        data['tokens'] = data['clean_text'].apply(lambda x: [word.lower() for word in x.split()]) #ensures all text is lowercase\n",
    "        print(\"Tokenisation successful\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenisation error: {e}\")\n",
    "        return None\n",
    "\n",
    "all_data = tokenise_text(all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I give permission for Wikipedia to use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aggression_parsed_dataset_subset.csv</td>\n",
       "      <td>give permission Wikipedia use material Nancy S...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>` October 2010 (UTC) :::::It does look that wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxicity_parsed_dataset_subset.csv</td>\n",
       "      <td>October 2010 UTC look way merger sounds like C...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sounds...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sound,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dcm81: #killerblondes ?? Have the producer...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>RT dcm81 killerblondes producers lost plot eye...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producers, lost, pl...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producer, lost, plo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ShaofHappiness COME SAY HI TO ME THIS EVENING...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>ShaofHappiness COME SAY HI EVENING ILU</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ManhattaKnight I mean he's gay, but he uses g...</td>\n",
       "      <td>1</td>\n",
       "      <td>cyberbullying_tweets_subset.csv</td>\n",
       "      <td>ManhattaKnight mean hes gay uses gendered slur...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  oh_label  \\\n",
       "0     I give permission for Wikipedia to use the ...         0   \n",
       "1  ` October 2010 (UTC) :::::It does look that wa...         0   \n",
       "2  RT @dcm81: #killerblondes ?? Have the producer...         0   \n",
       "3  @ShaofHappiness COME SAY HI TO ME THIS EVENING...         0   \n",
       "4  @ManhattaKnight I mean he's gay, but he uses g...         1   \n",
       "\n",
       "                                Dataset  \\\n",
       "0  aggression_parsed_dataset_subset.csv   \n",
       "1    toxicity_parsed_dataset_subset.csv   \n",
       "2     twitter_parsed_dataset_subset.csv   \n",
       "3     twitter_parsed_dataset_subset.csv   \n",
       "4       cyberbullying_tweets_subset.csv   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  give permission Wikipedia use material Nancy S...   \n",
       "1  October 2010 UTC look way merger sounds like C...   \n",
       "2  RT dcm81 killerblondes producers lost plot eye...   \n",
       "3             ShaofHappiness COME SAY HI EVENING ILU   \n",
       "4  ManhattaKnight mean hes gay uses gendered slur...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, 2010, utc, look, way, merger, sounds...   \n",
       "2  [rt, dcm81, killerblondes, producers, lost, pl...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [give, permission, wikipedia, use, material, n...  \n",
       "1  [october, 2010, utc, look, way, merger, sound,...  \n",
       "2  [rt, dcm81, killerblondes, producer, lost, plo...  \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]  \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def lemmatize_text(data):\n",
    "    \"\"\"\n",
    "    Lemmatises the tesxt data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lemm = WordNetLemmatizer() #using the inbuilt lemmatisation function\n",
    "\n",
    "    # Lemmatize all words\n",
    "        data['lemmatized'] = data['tokens'].apply(lambda x: [lemm.lemmatize(word) for word in x])\n",
    "        print(\"Lemmatisation successful\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during lemmatisation: {e}\")\n",
    "        return data\n",
    "\n",
    "all_data=lemmatize_text(data=all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_no_numbers</th>\n",
       "      <th>lemmatized_clean</th>\n",
       "      <th>tokenized_clean</th>\n",
       "      <th>string_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I give permission for Wikipedia to use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aggression_parsed_dataset_subset.csv</td>\n",
       "      <td>give permission Wikipedia use material Nancy S...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>[give, permission, wikipedia, use, material, n...</td>\n",
       "      <td>give permission wikipedia use material nancy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>` October 2010 (UTC) :::::It does look that wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxicity_parsed_dataset_subset.csv</td>\n",
       "      <td>October 2010 UTC look way merger sounds like C...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sounds...</td>\n",
       "      <td>[october, 2010, utc, look, way, merger, sound,...</td>\n",
       "      <td>[october, utc, look, way, merger, sound, like,...</td>\n",
       "      <td>[october, utc, look, way, merger, sound, like,...</td>\n",
       "      <td>[october, utc, look, way, merger, sounds, like...</td>\n",
       "      <td>october utc look way merger sounds like cathol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @dcm81: #killerblondes ?? Have the producer...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>RT dcm81 killerblondes producers lost plot eye...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producers, lost, pl...</td>\n",
       "      <td>[rt, dcm81, killerblondes, producer, lost, plo...</td>\n",
       "      <td>[rt, killerblondes, producer, lost, plot, eye,...</td>\n",
       "      <td>[rt, killerblondes, producer, lost, plot, eye,...</td>\n",
       "      <td>[rt, killerblondes, producers, lost, plot, eye...</td>\n",
       "      <td>rt killerblondes producers lost plot eyes fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ShaofHappiness COME SAY HI TO ME THIS EVENING...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter_parsed_dataset_subset.csv</td>\n",
       "      <td>ShaofHappiness COME SAY HI EVENING ILU</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>[shaofhappiness, come, say, hi, evening, ilu]</td>\n",
       "      <td>shaofhappiness come say hi evening ilu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ManhattaKnight I mean he's gay, but he uses g...</td>\n",
       "      <td>1</td>\n",
       "      <td>cyberbullying_tweets_subset.csv</td>\n",
       "      <td>ManhattaKnight mean hes gay uses gendered slur...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "      <td>[manhattaknight, mean, he, gay, us, gendered, ...</td>\n",
       "      <td>[manhattaknight, mean, hes, gay, uses, gendere...</td>\n",
       "      <td>manhattaknight mean hes gay uses gendered slur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  oh_label  \\\n",
       "0     I give permission for Wikipedia to use the ...         0   \n",
       "1  ` October 2010 (UTC) :::::It does look that wa...         0   \n",
       "2  RT @dcm81: #killerblondes ?? Have the producer...         0   \n",
       "3  @ShaofHappiness COME SAY HI TO ME THIS EVENING...         0   \n",
       "4  @ManhattaKnight I mean he's gay, but he uses g...         1   \n",
       "\n",
       "                                Dataset  \\\n",
       "0  aggression_parsed_dataset_subset.csv   \n",
       "1    toxicity_parsed_dataset_subset.csv   \n",
       "2     twitter_parsed_dataset_subset.csv   \n",
       "3     twitter_parsed_dataset_subset.csv   \n",
       "4       cyberbullying_tweets_subset.csv   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  give permission Wikipedia use material Nancy S...   \n",
       "1  October 2010 UTC look way merger sounds like C...   \n",
       "2  RT dcm81 killerblondes producers lost plot eye...   \n",
       "3             ShaofHappiness COME SAY HI EVENING ILU   \n",
       "4  ManhattaKnight mean hes gay uses gendered slur...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, 2010, utc, look, way, merger, sounds...   \n",
       "2  [rt, dcm81, killerblondes, producers, lost, pl...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, 2010, utc, look, way, merger, sound,...   \n",
       "2  [rt, dcm81, killerblondes, producer, lost, plo...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...   \n",
       "\n",
       "                               lemmatized_no_numbers  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, utc, look, way, merger, sound, like,...   \n",
       "2  [rt, killerblondes, producer, lost, plot, eye,...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...   \n",
       "\n",
       "                                    lemmatized_clean  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, utc, look, way, merger, sound, like,...   \n",
       "2  [rt, killerblondes, producer, lost, plot, eye,...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, he, gay, us, gendered, ...   \n",
       "\n",
       "                                     tokenized_clean  \\\n",
       "0  [give, permission, wikipedia, use, material, n...   \n",
       "1  [october, utc, look, way, merger, sounds, like...   \n",
       "2  [rt, killerblondes, producers, lost, plot, eye...   \n",
       "3      [shaofhappiness, come, say, hi, evening, ilu]   \n",
       "4  [manhattaknight, mean, hes, gay, uses, gendere...   \n",
       "\n",
       "                                    string_tokenized  \n",
       "0  give permission wikipedia use material nancy s...  \n",
       "1  october utc look way merger sounds like cathol...  \n",
       "2  rt killerblondes producers lost plot eyes fail...  \n",
       "3             shaofhappiness come say hi evening ilu  \n",
       "4  manhattaknight mean hes gay uses gendered slur...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(word_list):\n",
    "    \"\"\"\n",
    "    removees any numbers from the text\n",
    "    \"\"\"\n",
    "    return [word for word in word_list if not bool(re.search(r'\\d', word))]\n",
    "\n",
    "# Function to remove URLs from a list of words\n",
    "def remove_urls(word_list):\n",
    "    \"\"\"\n",
    "    Removes any URLs from the text\n",
    "    \"\"\"\n",
    "    return [word for word in word_list if not (word.startswith('http') or word.startswith('www') or word.startswith('https'))]\n",
    "\n",
    "all_data['lemmatized_no_numbers'] = all_data['lemmatized'].apply(remove_numbers)\n",
    "all_data['lemmatized_clean'] = all_data['lemmatized_no_numbers'].apply(remove_urls)\n",
    "\n",
    "all_data['tokenized_clean']=all_data['tokens'].apply(remove_numbers)\n",
    "all_data['tokenized_clean']=all_data['tokenized_clean'].apply(remove_urls)\n",
    "all_data['string_tokenized']=all_data['tokenized_clean'].apply(' '.join)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with oh_label 1: 3736\n",
      "Number of rows with oh_label 0: 10754\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows where 'oh_label' is 1\n",
    "count_label_1 = all_data[all_data['oh_label'] == 1].shape[0]\n",
    "\n",
    "# Count the number of rows where 'oh_label' is 0\n",
    "count_label_0 = all_data[all_data['oh_label'] == 0].shape[0]\n",
    "\n",
    "print(f\"Number of rows with oh_label 1: {count_label_1}\")\n",
    "print(f\"Number of rows with oh_label 0: {count_label_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the lemmatized strings\n",
    "def tokenize_texts(texts, max_length):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokenized_texts\n",
    "\n",
    "# Tokenize train and test texts\n",
    "\n",
    "max_length = 128\n",
    "train_tokenized_texts = tokenize_texts(train_data['string_tokenized'].tolist(), max_length)\n",
    "test_tokenized_texts = tokenize_texts(test_data['string_tokenized'].tolist(), max_length)\n",
    "\n",
    "# Prepare train and test inputs and labels\n",
    "train_inputs = {\n",
    "    'input_ids': train_tokenized_texts['input_ids'],\n",
    "    'attention_mask': train_tokenized_texts['attention_mask']\n",
    "}\n",
    "train_labels = torch.tensor(train_data['oh_label'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_inputs = {\n",
    "    'input_ids': test_tokenized_texts['input_ids'],\n",
    "    'attention_mask': test_tokenized_texts['attention_mask']\n",
    "}\n",
    "test_labels = torch.tensor(test_data['oh_label'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Transformer model\n",
    "class CyberBullyingClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CyberBullyingClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        probability = self.sigmoid(logits)\n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CyberBullyingClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model instance\n",
    "model = CyberBullyingClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "epochs = 100\n",
    "total_steps = len(train_inputs['input_ids']) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Defining loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_inputs, train_labels, criterion, optimizer, scheduler, device, batch_size, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m train_labels[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mCyberBullyingClassifier.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[0;32m     16\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1073\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1071\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m-> 1073\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1082\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:211\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m--> 211\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_type_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_inputs, train_labels, criterion, optimizer, scheduler, device, batch_size=32, epochs=epochs):\n",
    "    model.train()\n",
    "    best_loss=float('inf')\n",
    "    wait=0\n",
    "    epsilon=1e-8\n",
    "    patience_counter=3\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, len(train_inputs['input_ids']), batch_size):\n",
    "            inputs = {key: val[i:i+batch_size].to(device) for key, val in train_inputs.items()}\n",
    "            labels = train_labels[i:i+batch_size].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_size\n",
    "\n",
    "        epoch_loss = running_loss / len(train_labels)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        if epoch_loss<best_loss-epsilon:\n",
    "            best_loss=epoch_loss\n",
    "            wait=0\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait >=patience_counter:\n",
    "                print(f'Stopping early at epoch {epoch+1} due to insignificant loss change.')\n",
    "                break\n",
    "\n",
    "# Training the model\n",
    "train_model(model, train_inputs, train_labels, criterion, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate_model(model, test_inputs, test_labels, device, batch_size=32):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_inputs['input_ids']), batch_size):\n",
    "            inputs = {key: val[i:i+batch_size].to(device) for key, val in test_inputs.items()}\n",
    "            labels = test_labels[i:i+batch_size].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    preds = np.array(preds) >= 0.5\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = evaluate_model(model, test_inputs, test_labels, device)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), '/home/ara2/Desktop/Farhan_Bullishield_CSE498R/Created Models/english_bert_class_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def sentence_preprocessing(sentence_list):\n",
    "    '''Returns a list of lemmatized texts upon calling'''\n",
    "    \n",
    "    # initialize stopwords for english\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    preprocessed_sentence_list=[]\n",
    "    for sentence in sentence_list:\n",
    "        # keep the type str\n",
    "        sentence=str(sentence)\n",
    "        \n",
    "        # lowercasing the sentence and tokenising it\n",
    "        words= word_tokenize(sentence.lower())\n",
    "        \n",
    "        # Filter out stopwords\n",
    "        filtered_tokenized_words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # lemmatize texts\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        lemmatized_words=[lemmatizer.lemmatize(word) for word in filtered_tokenized_words] \n",
    "        \n",
    "\n",
    "        # remove urls and numbers\n",
    "        clean_sentence=remove_numbers(word_list=lemmatized_words)\n",
    "        preprocessed_sentence=remove_urls(word_list=clean_sentence)\n",
    "        preprocessed_sentence_list.append(preprocessed_sentence)\n",
    "    \n",
    "    return preprocessed_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(preprocessed_sentence_list):\n",
    "    \n",
    "    sentiment_score_list=[]\n",
    "    intensity_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for i in range(len(preprocessed_sentence_list)):\n",
    "        # convert list of words to string\n",
    "        converted_string=\" \".join(preprocessed_sentence_list[i])\n",
    "        # analyze sentiment\n",
    "        \n",
    "        sentiment_score=intensity_analyser.polarity_scores(converted_string)['compound']\n",
    "        sentiment_score_list.append(sentiment_score)\n",
    "        \n",
    "    return sentiment_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check score\n",
    "\n",
    "sentence_list=[\"Alvi is really a good motherfucker\",\"Fuck you motherfucker\",\"I have loved you all my life\"]\n",
    "\n",
    "preprocessed_list=sentence_preprocessing(sentence_list)\n",
    "sentiment_list=analyze_sentiment(preprocessed_list)\n",
    "\n",
    "for i in range(len(sentence_list)):\n",
    "    print(f\"Sentence:{sentence_list[i]}\\nSentiment Score:{sentiment_list[i]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_custom_texts(model, tokenizer, texts, device):\n",
    "    model.eval()\n",
    "    tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {key: val.to(device) for key, val in tokenized_texts.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = outputs.cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "# Predict custom texts\n",
    "custom_probabilities = predict_custom_texts(model, tokenizer, sentence_list, device)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "custom_pred_labels = [1 if prob >= 0.5 else 0 for prob in custom_probabilities]\n",
    "\n",
    "# Print predictions\n",
    "for text, label in zip(sentence_list, custom_pred_labels):\n",
    "    if label == 1:\n",
    "        print(f'Text: \"{text}\" is predicted as cyberbullying.')\n",
    "    else:\n",
    "        print(f'Text: \"{text}\" is predicted as not cyberbullying.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
