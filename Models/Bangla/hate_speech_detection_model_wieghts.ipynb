{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield Models\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import bangla_nlp\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Bengali hate speech .csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hate-Non hate distribution\")\n",
    "df['hate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered=df[df['hate']==1]\n",
    "print(\"Number of hate comments in every category\")\n",
    "df_filtered['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove punctuations\n",
    "df['clean_punctuation']=df['sentence'].apply(bangla_nlp.clean_punctuations)\n",
    "df['clean_emoji']=df['clean_punctuation'].apply(bangla_nlp.clean_emoji)\n",
    "df['clean_text']=df['clean_emoji'].apply(bangla_nlp.clean_url_and_email)\n",
    "df['clean_text']=df['clean_text'].apply(bangla_nlp.clean_digits)\n",
    "drop_columns=['clean_punctuation','clean_emoji']\n",
    "df.drop(columns=drop_columns,inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_words']=df['clean_text'].apply(bangla_nlp.word_tokenize_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_stopword_tokenized_words']=df['tokenized_words'].apply(bangla_nlp.remove_stopwords_from_tokens)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts=df['hate'].value_counts()\n",
    "# Plot a pie chart\n",
    "value_counts.plot.pie(autopct='%.1f%%')\n",
    "# Add a title\n",
    "plt.title(\"Data Distribution Pie Chart\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "df['hate'].value_counts().plot(kind='barh', figsize=(9, 3))\n",
    "plt.xlabel(\"Number of Comments\", labelpad=12)\n",
    "plt.ylabel(\"Sentiment Class\", labelpad=12)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title(\"Data Distribution\", y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Review of each of the Review\n",
    "df['ReviewLength'] = df.sentence.apply(lambda x:len(x.split()))\n",
    "frequency = dict()\n",
    "for i in df.ReviewLength:\n",
    "    frequency[i] = frequency.get(i, 0)+1\n",
    "\n",
    "plt.bar(frequency.keys(), frequency.values(), color =\"b\")\n",
    "plt.xlim(1, 135)\n",
    "# in this notbook color is not working but it should work.\n",
    "plt.xlabel('Lenght of the Texts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Length-Frequency Distribution')\n",
    "plt.show()\n",
    "print(f\"Maximum Length of a review: {max(df.ReviewLength)}\")\n",
    "print(f\"Minimum Length of a review: {min(df.ReviewLength)}\")\n",
    "print(f\"Average Length of a reviews: {round(np.mean(df.ReviewLength),0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of samples in each class\n",
    "class_counts = df['hate'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_texts(texts, max_length):\n",
    "    tokenizer = BertTokenizer.from_pretrained('sagorsarker/bangla-bert-base')\n",
    "    tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokenized_texts\n",
    "\n",
    "max_length=128\n",
    "\n",
    "train_tokenized_texts = tokenize_texts(train_data['clean_string_words'].tolist(), max_length)\n",
    "test_tokenized_texts = tokenize_texts(train_data['clean_string_words'].tolist(), max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test inputs and labels\n",
    "train_inputs = {\n",
    "    'input_ids': train_tokenized_texts['input_ids'],\n",
    "    'attention_mask': train_tokenized_texts['attention_mask']\n",
    "}\n",
    "\n",
    "train_labels = torch.tensor(train_data['hate'].values,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_inputs = {\n",
    "    'input_ids': test_tokenized_texts['input_ids'],\n",
    "    'attention_mask': test_tokenized_texts['attention_mask']\n",
    "}\n",
    "\n",
    "test_labels = torch.tensor(test_data['hate'].values,dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels.cpu().numpy()),\n",
    "    y=train_labels.cpu().numpy()\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bangla Bert model\n",
    "class CyberBullyingClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CyberBullyingClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('sagorsarker/bangla-bert-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        probability = self.sigmoid(logits)\n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "model = CyberBullyingClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "epochs = 100\n",
    "total_steps = len(train_inputs['input_ids']) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Update the loss function to use class weights\n",
    "criterion = nn.BCELoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_inputs, train_labels, criterion, optimizer, scheduler, device, batch_size=32, epochs=epochs):\n",
    "    model.train()\n",
    "    best_loss=float('inf')\n",
    "    wait=0\n",
    "    epsilon=1e-8\n",
    "    patience_counter=3\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, len(train_inputs['input_ids']), batch_size):\n",
    "            inputs = {key: val[i:i+batch_size].to(device) for key, val in train_inputs.items()}\n",
    "            labels = train_labels[i:i+batch_size].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_size\n",
    "\n",
    "        epoch_loss = running_loss / len(train_labels)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        if epoch_loss<best_loss-epsilon:\n",
    "            best_loss=epoch_loss\n",
    "            wait=0\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait >=patience_counter:\n",
    "                print(f'Stopping early at epoch {epoch+1} due to insignificant loss change.')\n",
    "                break\n",
    "\n",
    "# Training the model\n",
    "train_model(model, train_inputs, train_labels, criterion, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'G:\\OneDrive - northsouth.edu\\CODES\\PROJECTS\\PROJECT - Bullishield\\Created Models/bangla_bert_class_imbalanced_wieghts.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
